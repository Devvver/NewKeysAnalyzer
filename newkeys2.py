import streamlit as st
import pandas as pd
import datetime
import requests
import time
import os
from urllib.parse import urlparse
from xml.etree import ElementTree
from google.oauth2 import service_account
from googleapiclient.discovery import build


# --- –§–£–ù–ö–¶–ò–ò API ---

def get_gsc_service_sa():
    """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Service Account"""
    try:
        if not os.path.exists('credentials.json'):
            st.error("‚ùå –§–∞–π–ª credentials.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            st.stop()
        creds = service_account.Credentials.from_service_account_file(
            'credentials.json',
            scopes=['https://www.googleapis.com/auth/webmasters.readonly']
        )
        return build('searchconsole', 'v1', credentials=creds)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        st.stop()


def get_urls_from_sitemap(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ URL –∏–∑ Sitemap XML"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        res = requests.get(url, headers=headers, timeout=20)
        res.raise_for_status()
        tree = ElementTree.fromstring(res.content)
        ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        urls = [loc.text for loc in tree.findall(".//ns:loc", ns)]
        return urls
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Sitemap: {e}")
        return []


def fetch_detailed_keys(service, site, page, start, end):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π —Å –ø–æ–∫–∞–∑–∞–º–∏, –∫–ª–∏–∫–∞–º–∏ –∏ –ø–æ–∑–∏—Ü–∏—è–º–∏"""
    body = {
        'startDate': start,
        'endDate': end,
        'dimensions': ['query'],
        'dimensionFilterGroups': [{
            'filters': [{
                'dimension': 'page',
                'operator': 'equals',
                'expression': page
            }]
        }],
        'rowLimit': 5000
    }
    try:
        response = service.searchanalytics().query(siteUrl=site, body=body).execute()
        rows = response.get('rows', [])
        # –û–∫—Ä—É–≥–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –¥–æ –¥–µ—Å—è—Ç—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö
        return {r['keys'][0]: {
            'clicks': int(r['clicks']),
            'impressions': int(r['impressions']),
            'position': round(r['position'], 1)
        } for r in rows}
    except Exception:
        return {}


def get_month_range(year, month_idx):
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≥–æ–¥ –∏ –º–µ—Å—è—Ü –≤ –¥–∞—Ç—ã –ì–ì–ì–ì-–ú–ú-–î–î –¥–ª—è API"""
    start_date = datetime.date(year, month_idx, 1)
    if month_idx == 12:
        end_date = datetime.date(year + 1, 1, 1) - datetime.timedelta(days=1)
    else:
        end_date = datetime.date(year, month_idx + 1, 1) - datetime.timedelta(days=1)
    return start_date.isoformat(), end_date.isoformat()


# --- –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

st.set_page_config(page_title="GSC New Keys Analyzer PRO", layout="wide")
st.title("üöÄ –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∫–ª—é—á–µ–π")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (—á—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø—Ä–æ–ø–∞–¥–∞–ª)
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None

MONTHS_LIST = ["–Ø–Ω–≤–∞—Ä—å", "–§–µ–≤—Ä–∞–ª—å", "–ú–∞—Ä—Ç", "–ê–ø—Ä–µ–ª—å", "–ú–∞–π", "–ò—é–Ω—å",
               "–ò—é–ª—å", "–ê–≤–≥—É—Å—Ç", "–°–µ–Ω—Ç—è–±—Ä—å", "–û–∫—Ç—è–±—Ä—å", "–ù–æ—è–±—Ä—å", "–î–µ–∫–∞–±—Ä—å"]
MONTHS_DICT = {name: i + 1 for i, name in enumerate(MONTHS_LIST)}

with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    sitemap_url = st.text_input("URL Sitemap.xml", placeholder="https://example.com/sitemap.xml")

    st.divider()
    st.subheader("–í—ã–±–æ—Ä –ø–µ—Ä–∏–æ–¥–æ–≤")

    today = datetime.date.today()
    # –õ–æ–≥–∏–∫–∞ –¥–µ—Ñ–æ–ª—Ç–æ–≤ (–Ø–Ω–≤–∞—Ä—å vs –î–µ–∫–∞–±—Ä—å, –µ—Å–ª–∏ —Å–µ–≥–æ–¥–Ω—è –§–µ–≤—Ä–∞–ª—å)
    first_day_this_month = today.replace(day=1)
    last_month_dt = first_day_this_month - datetime.timedelta(days=1)
    prev_month_dt = last_month_dt.replace(day=1) - datetime.timedelta(days=1)

    # 1. –ú–µ—Å—è—Ü –∞–Ω–∞–ª–∏–∑–∞
    st.write("**–ú–µ—Å—è—Ü –∞–Ω–∞–ª–∏–∑–∞:**")
    col1_y, col1_m = st.columns(2)
    with col1_y:
        y1 = st.selectbox("–ì–æ–¥", [today.year, today.year - 1],
                          index=0 if last_month_dt.year == today.year else 1, key="y1")
    with col1_m:
        m1 = st.selectbox("–ú–µ—Å—è—Ü", MONTHS_LIST, index=last_month_dt.month - 1, key="m1")

    # 2. –ë–∞–∑–æ–≤—ã–π –º–µ—Å—è—Ü
    st.write("**–° —á–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º (–ë–∞–∑–∞):**")
    col2_y, col2_m = st.columns(2)
    with col2_y:
        y2 = st.selectbox("–ì–æ–¥ –±–∞–∑—ã", [today.year, today.year - 1],
                          index=0 if prev_month_dt.year == today.year else 1, key="y2")
    with col2_m:
        m2 = st.selectbox("–ú–µ—Å—è—Ü –±–∞–∑—ã", MONTHS_LIST, index=prev_month_dt.month - 1, key="m2")

    st.info("–í—ã–±—Ä–∞–Ω—ã –ø—Ä–æ—à–ª—ã–π –∏ –ø–æ–∑–∞–ø—Ä–æ—à–ª—ã–π –º–µ—Å—è—Ü—ã –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö GSC.")

if st.button(" –ü–∞—Ä—Å–∏–Ω–≥"):
    if not sitemap_url:
        st.warning("–í–≤–µ–¥–∏—Ç–µ URL Sitemap!")
    else:
        service = get_gsc_service_sa()
        all_urls = get_urls_from_sitemap(sitemap_url)

        if not all_urls:
            st.error("Sitemap –ø—É—Å—Ç –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
        else:
            # –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Å—ã–ª–∫–∏ Sitemap
            parsed_uri = urlparse(all_urls[0])
            site_url = f'{parsed_uri.scheme}://{parsed_uri.netloc}/'


            cur_start, cur_end = get_month_range(y1, MONTHS_DICT[m1])
            prev_start, prev_end = get_month_range(y2, MONTHS_DICT[m2])

            total = len(all_urls)
            progress_bar = st.progress(0)
            status_text = st.empty()
            temp_results = []

            for i, url in enumerate(all_urls):
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {i + 1}/{total}")

                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –¥–≤–∞ –ø–µ—Ä–∏–æ–¥–∞
                data_now = fetch_detailed_keys(service, site_url, url, cur_start, cur_end)
                data_prev = fetch_detailed_keys(service, site_url, url, prev_start, prev_end)

                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
                new_queries = set(data_now.keys()) - set(data_prev.keys())

                if new_queries:
                    metrics_list = []
                    for q in new_queries:
                        m = data_now[q]
                        metrics_list.append({
                            "–ó–∞–ø—Ä–æ—Å": q,
                            "–ü–æ–∫–∞–∑—ã": m['impressions'],
                            "–ö–ª–∏–∫–∏": m['clicks'],
                            "–ü–æ–∑–∏—Ü–∏—è": m['position']
                        })

                    temp_results.append({
                        "URL": url,
                        "Count": len(new_queries),
                        "Metrics": metrics_list
                    })

                progress_bar.progress((i + 1) / total)
                if i % 10 == 0:
                    time.sleep(0.02)

            status_text.empty()
            st.session_state.analysis_results = temp_results

# --- –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---

if st.session_state.analysis_results:
    res_list = st.session_state.analysis_results
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª-–≤—É –Ω–æ–≤—ã—Ö –∫–ª—é—á–µ–π (–¢–æ–ø-50)
    sorted_res = sorted(res_list, key=lambda x: x['Count'], reverse=True)[:50]

    st.divider()
    st.subheader(f"üî• –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞: {m1} {y1} –ø—Ä–æ—Ç–∏–≤ {m2} {y2}")

    for idx, row in enumerate(sorted_res):
        with st.expander(f"‚ûï {row['Count']} –Ω–æ–≤—ã—Ö ‚Äî {row['URL']}"):
            df = pd.DataFrame(row['Metrics'])
            df = df.sort_values(by="–ü–æ–∫–∞–∑—ã", ascending=False)

            st.write("**–ú–µ—Ç—Ä–∏–∫–∏ –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:**")

            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–≤–æ–¥–∞: –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ –¥–ª—è –ø–æ–∑–∏—Ü–∏–π
            st.table(df.style.format({
                "–ü–æ–∑–∏—Ü–∏—è": "{:.1f}",
                "–ü–æ–∫–∞–∑—ã": "{:,.0f}",
                "–ö–ª–∏–∫–∏": "{:,.0f}"
            }))

            st.write("**–°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (–¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è):**")
            clean_keys = "\n".join(df["–ó–∞–ø—Ä–æ—Å"].tolist())
            st.text_area(label="–¢–µ–∫—Å—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫:", value=clean_keys, height=180, key=f"txt_{idx}")
            st.divider()